# PS-03 -- Product Domain Verification and Classification Pipeline
# ğŸ“Œ Overview

This project implements an end-to-end Machine Learning pipeline to classify products into their respective business / industry domains (e.g., FinTech, AgriTech, Cybersecurity) using product names and descriptions.

# ğŸ¯ Problem Statement
E-commerce and enterprise product catalogs often contain noisy or ambiguous product information. Accurately identifying the correct domain of a product is critical for:
-Search and recommendation systems

-Product analytics

-Catalog organization

-Automated decision systems

This project solves the problem by training a text-based domain classifier using curated and generated product data.

# ğŸ§  Solution Approach
The pipeline follows a structured ML workflow:
1. Data Generation & Collection
Synthetic and real-world product data are combined to improve domain coverage.

2. Data Cleaning & Preparation
Missing values are handled and relevant textual features are extracted.

3. Text Vectorization
Product descriptions are converted into numerical representations suitable for ML models.

4. Model Training
A supervised classification model is trained to predict product domains.

5. Model Persistence
The trained model is serialized and stored for reuse in inference pipelines.

# ğŸ—‚ï¸ Project Structure
PS-03/

â”‚

â”œâ”€â”€ .venv/ # Python virtual environment (ignored in Git)

â”‚

â”œâ”€â”€ data/ # Datasets

â”‚ â”œâ”€â”€ combined_products.csv

â”‚ â”œâ”€â”€ flipkart_cleaned_data.csv

â”‚ â””â”€â”€ generated_products.csv

â”‚

â”œâ”€â”€ model/ # Trained ML models

â”‚ â””â”€â”€ flipkart_domain_classifier.pkl

â”‚

â”œâ”€â”€ notebooks/ # Experiments & analysis

â”‚ â””â”€â”€ ml_pipeline.ipynb

â”‚

â”œâ”€â”€ src/ # Source code

â”‚ â”œâ”€â”€ concatenate.py

â”‚ â”œâ”€â”€ data_generation.py

â”‚ â”œâ”€â”€ domain_name_classifier.py

â”‚ 

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ .gitignore

â”œâ”€â”€ .gitattributes

â””â”€â”€ README.md

# âš™ï¸ Environment Setup
1ï¸âƒ£ Create virtual environment

python -m venv .venv

2ï¸âƒ£ Activate environment

# Windows

.venv\Scripts\activate

# macOS / Linux

source .venv/bin/activate

3ï¸âƒ£ Install dependencies

pip install -r requirements.txt

# ğŸš€ Training the Model

Run the training pipeline from the project root:

python src/train_classifier.py

This will:

-Load and preprocess training data

-Train the domain classification model

-Save the trained model to the model/ directory

#ğŸ” Using the Trained Model (Inference)

import joblib

model = joblib.load("model/flipkart_domain_classifier.pkl")

prediction = model.predict(["Biometric fingerprint authentication device"])

print(prediction)

âš ï¸ Ensure the same preprocessing logic and library versions are used during inference.

# ğŸ“¦ Dependencies

Key libraries used:

Python 3.x

pandas

scikit-learn

joblib

sentence-transformers 

All dependencies are pinned in requirements.txt for reproducibility.

# ğŸ§ª Experiments

Exploratory analysis and rapid experimentation are performed in Jupyter notebooks located in the notebooks/ directory.
Notebooks are not considered production code.

# ğŸ›¡ï¸ Reproducibility & Best Practices

One virtual environment per project

Frozen dependencies using requirements.txt

Modular pipeline design

Separation of data, code, and models

# ğŸ“ˆ Future Improvements

Add deep learningâ€“based text embeddings

Introduce model versioning

Add evaluation metrics logging

Expose inference via REST API
